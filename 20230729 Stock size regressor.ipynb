{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnxT0MUUdSoq+2K4jv0v4u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jonschth/mlmodels/blob/main/20230729%20Stock%20size%20regressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj19US7a4073"
      },
      "outputs": [],
      "source": [
        "\"\"\"Optimal regression found to estimate catch sizes.\"\"\"\n",
        "import pandas as pd\n",
        "import math\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.pipeline\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import xgboost as xgb\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import shap\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "\n",
        "path_str = 'R:\\\\Ráðgjöf\\\\Maris Optimum/gr_data\\\\'\n",
        "# path_str = ''\n",
        "\n",
        "\n",
        "def get_new_data(fractile):\n",
        "    \"\"\"\n",
        "    Fetch all data for the regression.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    fractile : integer.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    XX_df : Dataframe with data for all the dependent variables.\n",
        "    YY : Dataframe with data for the the independent variable.\n",
        "\n",
        "    \"\"\"\n",
        "    X100_df = pd.read_csv(path_str + 'distribution100.csv',\n",
        "                          sep=\",\")\n",
        "    ysq_df = X100_df[['ar', 'max(cum)']].copy()\n",
        "    ysq_df.set_index(['ar'], inplace=True)\n",
        "    ysq_df = ysq_df[~ysq_df.index.duplicated(keep='first')]\n",
        "\n",
        "    X_df = pd.read_csv(path_str + 'distribution' + fractile + '.csv',\n",
        "                       sep=\",\")\n",
        "    X_temperature = pd.read_csv(path_str + 'distribution_temp.csv',\n",
        "                       sep=\",\")\n",
        "\n",
        "    catch_df = pd.read_csv(path_str + 'golden_redfish_catch.csv',\n",
        "                           sep=\";\")\n",
        "\n",
        "    catch_df.at[37, 'year'] = 2022.0\n",
        "    catch_df.at[37, 'catch'] = 26\n",
        "    catch_df.at[37, 'number'] = 29\n",
        "\n",
        "    X_cal_df = pd.read_csv(path_str + 'distribution_commercial.csv',\n",
        "                           sep=\",\")\n",
        "    X_cal_df.drop(1605, axis=0, inplace=True)\n",
        "\n",
        "    X_cal_df = X_cal_df.pivot(index='ar',\n",
        "                              columns='lengd',\n",
        "                              values='per_length')\n",
        "    X_cal_df = X_cal_df.fillna(0)\n",
        "    X_cal_df.columns = 1000 + X_cal_df.columns\n",
        "    X_cal_df.columns = X_cal_df.columns.astype(int).astype(str)\n",
        "\n",
        "    catch_df = catch_converter(X_cal_df, catch_df)\n",
        "    catch_df.year = catch_df.year.astype(int)\n",
        "    catch_df.set_index(catch_df.year, inplace=True)\n",
        "\n",
        "    X_cal_df = X_cal_df.mul(catch_df.number*-1e6, axis=0)\n",
        "\n",
        "    XX_df = X_df.pivot(index='ar',\n",
        "                       columns='lengd',\n",
        "                       values='per_length')\n",
        "\n",
        "    XX_df = pd.merge(XX_df, ysq_df, right_index=True, left_index=True)\n",
        "    XX_df = pd.merge(XX_df, X_temperature['bottom_temp'],right_index=True, left_index=True)\n",
        "\n",
        "    XX_df.drop(11.9, axis=1, inplace=True)\n",
        "    XX_df.drop(12.5, axis=1, inplace=True)\n",
        "    XX_df.drop(12.6, axis=1, inplace=True)\n",
        "    XX_df.drop(13.1, axis=1, inplace=True)\n",
        "    XX_df.drop(13.4, axis=1, inplace=True)\n",
        "    XX_df.drop(13.6, axis=1, inplace=True)\n",
        "    XX_df.drop(13.7, axis=1, inplace=True)\n",
        "    XX_df.drop(13.9, axis=1, inplace=True)\n",
        "    XX_df.drop(14.4, axis=1, inplace=True)\n",
        "    XX_df.drop(14.5, axis=1, inplace=True)\n",
        "    XX_df.drop(14.7, axis=1, inplace=True)\n",
        "    XX_df.drop(14.8, axis=1, inplace=True)\n",
        "    XX_df.drop(14.9, axis=1, inplace=True)\n",
        "\n",
        "    XX_df.columns = XX_df.columns.astype(str)\n",
        "\n",
        "    YX = pd.read_csv(path_str+\"RED_numbers_at_age.csv\", sep=\";\")\n",
        "    YY = YX.iloc[15:53, 28]\n",
        "\n",
        "    YZ = pd.read_csv(\"R:\\\\Ráðgjöf\\\\Maris Optimum\\\\stock_measurement\\\\stock_measurement.csv\", sep=\";\")\n",
        "    YY = YZ.iloc[:,1]\n",
        "\n",
        "\n",
        "    # XX_df = XX_df.join(X_cal_df.iloc[:, :])\n",
        "\n",
        "    XX_df = XX_df.join(catch_df.loc[:, 'number'])\n",
        "    XX_df.index = XX_df.index.astype(str)\n",
        "\n",
        "    '''s = XX_df.index[29:35]\n",
        "    s.index = XX_df[29:35]\n",
        "    s = pd.get_dummies(s)\n",
        "    s.index = XX_df.index[29:35]\n",
        "    XX_df = XX_df.join(s)'''\n",
        "\n",
        "    XX_df = XX_df.fillna(0)\n",
        "    heat_maps(XX_df, YY)\n",
        "    return (XX_df, YY)\n",
        "\n",
        "def stock_weight_converter(X_catch_per_df, catch_df):\n",
        "    \"\"\"\n",
        "    Stock_weight information used to formulate and calculate units from kg.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_catch_per_df : Dataframe containing percentages of catch.\n",
        "    stoxk_df : Dataframe containing total stock in kg.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    catch_df : Dataframe containing total catch in lengths.\n",
        "    \"\"\"\n",
        "    path_grs_str = 'R:/Ráðgjöf/Maris Optimum/Golden_redfish_model/'\n",
        "    path_grs_weight =\\\n",
        "        'R:/Ráðgjöf/Maris Optimum/Data_from_Hafro_20220906/ssb_weights/'\n",
        "\n",
        "    red_length_weights = pd.read_csv(\n",
        "        path_grs_weight + \"red_length_weights.csv\")\n",
        "    red_length_weights.set_index('ar', inplace=True)\n",
        "\n",
        "    wl_df = pd.read_csv(path_grs_str+'RED_gadget_n_at_age.csv', sep=',')\n",
        "\n",
        "    Xl = wl_df[['year', 'mean_length']].copy()\n",
        "    yl = wl_df[['year', 'mean_weight']].copy()\n",
        "\n",
        "    for index, row in Xl.iterrows():\n",
        "        Xl.at[index, 'squared'] = row[1]**2\n",
        "\n",
        "    for year in range(1985, 2022):\n",
        "        average_weight = 0\n",
        "        reg_X = Xl[Xl['year'] == year]\n",
        "        reg_y = yl[yl['year'] == year]\n",
        "        reg = LinearRegression().fit(\n",
        "            reg_X[['mean_length', 'squared']], reg_y[['mean_weight']])\n",
        "        b = reg.coef_[0][0]\n",
        "        a = reg.coef_[0][1]\n",
        "        c = reg.intercept_\n",
        "        for col in range(1020, 1060):\n",
        "            if year > 1995 and \\\n",
        "               pd.isna(red_length_weights.loc[year, str(col-1000)]) is False:\n",
        "                average_weight += 1/1000*(X_catch_per_df.loc[year, str(col)]) * red_length_weights.loc[year, str(col-1000)]\n",
        "            else:\n",
        "                average_weight += (X_catch_per_df.loc[year, str(col)]) * (\n",
        "                    a*(col - 1000)**2 + b*(col - 1000) + c)\n",
        "        catch_df.at[\n",
        "            year - 1985, 'number'] = catch_df.loc[\n",
        "                year - 1985, 'catch']/average_weight\n",
        "\n",
        "    df = red_length_weights\n",
        "    scaled_df = (df - df.min(axis=0))/(df.max(axis=0) - df.min(axis=0))\n",
        "    ax = sns.heatmap(scaled_df, linewidths=.5, cmap='RdYlGn')\n",
        "    ax.set(ylabel='year')\n",
        "    plt.show()\n",
        "    ax=sns\n",
        "    return catch_df\n",
        "\n",
        "def catch_converter(X_catch_per_df, catch_df):\n",
        "    \"\"\"\n",
        "    Catch information used to formulate and calculate units from kg.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_catch_per_df : Dataframe containing percentages of catch.\n",
        "    catch_df : Dataframe containing total cath in kg.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    catch_df : Dataframe containing total catch in lengths.\n",
        "    \"\"\"\n",
        "    path_grs_str = 'R:/Ráðgjöf/Maris Optimum/Golden_redfish_model/'\n",
        "    path_grs_weight =\\\n",
        "        'R:/Ráðgjöf/Maris Optimum/Data_from_Hafro_20220906/ssb_weights/'\n",
        "\n",
        "    red_length_weights = pd.read_csv(\n",
        "        path_grs_weight + \"red_length_weights.csv\")\n",
        "    red_length_weights.set_index('ar', inplace=True)\n",
        "\n",
        "    wl_df = pd.read_csv(path_grs_str+'RED_gadget_n_at_age.csv', sep=',')\n",
        "\n",
        "    Xl = wl_df[['year', 'mean_length']].copy()\n",
        "    yl = wl_df[['year', 'mean_weight']].copy()\n",
        "\n",
        "    for index, row in Xl.iterrows():\n",
        "        Xl.at[index, 'squared'] = row[1]**2\n",
        "\n",
        "    for year in range(1985, 2022):\n",
        "        average_weight = 0\n",
        "        reg_X = Xl[Xl['year'] == year]\n",
        "        reg_y = yl[yl['year'] == year]\n",
        "        reg = LinearRegression().fit(\n",
        "            reg_X[['mean_length', 'squared']], reg_y[['mean_weight']])\n",
        "        b = reg.coef_[0][0]\n",
        "        a = reg.coef_[0][1]\n",
        "        c = reg.intercept_\n",
        "        for col in range(1020, 1060):\n",
        "            if year > 1995 and \\\n",
        "               pd.isna(red_length_weights.loc[year, str(col-1000)]) is False:\n",
        "                average_weight += 1/1000*(X_catch_per_df.loc[year, str(col)]) * red_length_weights.loc[year, str(col-1000)]\n",
        "            else:\n",
        "                average_weight += (X_catch_per_df.loc[year, str(col)]) * (\n",
        "                    a*(col - 1000)**2 + b*(col - 1000) + c)\n",
        "        catch_df.at[\n",
        "            year - 1985, 'number'] = catch_df.loc[\n",
        "                year - 1985, 'catch']/average_weight\n",
        "\n",
        "    df = red_length_weights\n",
        "    scaled_df = (df - df.min(axis=0))/(df.max(axis=0) - df.min(axis=0))\n",
        "    ax = sns.heatmap(scaled_df, linewidths=.5, cmap='RdYlGn')\n",
        "    ax.set(ylabel='year')\n",
        "    plt.show()\n",
        "    return catch_df\n",
        "\n",
        "def heat_maps(X, y):\n",
        "    \"\"\"\n",
        "    Plot heatmaps.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : Independent variables.\n",
        "    y : Dependent variables.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Pictues of heat maps.\n",
        "\n",
        "    \"\"\"\n",
        "    sns.color_palette(\"vlag\", as_cmap=True)\n",
        "    ax = sns.heatmap(X.iloc[:, :50], cmap='RdYlGn')\n",
        "    ax.set(ylabel='year')\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    ax = sns.heatmap((\n",
        "        y.to_numpy() * X.iloc[:, 0:50].T).T, cmap='RdYlGn')\n",
        "    ax.set(ylabel='year')\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.show()\n",
        "\n",
        "def fitting_plot(y_test, y_pred_test, X_test, xgb_regressor):\n",
        "    \"\"\"\n",
        "    Plot how well predictions fit the independent test variables.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_test : independent y variables.\n",
        "    y_pred_test : independent predictions.\n",
        "    X_test : datframe containing the dependent test variables.\n",
        "    xgb_regressor : the optimal regressor.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None.\n",
        "\n",
        "    \"\"\"\n",
        "    x_ax = range(len(y_test))\n",
        "    plt.scatter(x_ax,\n",
        "                y_test,\n",
        "                s=5,\n",
        "                color=\"blue\",\n",
        "                label=\"original\")\n",
        "    plt.plot(x_ax,\n",
        "             y_pred_test,\n",
        "             lw=0.8,\n",
        "             color=\"red\",\n",
        "             label=\"predicted\")\n",
        "    plt.xticks(x_ax,\n",
        "               X_test.index,\n",
        "               rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "def error_plot(regressor):\n",
        "    \"\"\"\n",
        "    Plot the accuracy of the model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    regressor : Xgboost regressor.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None.\n",
        "\n",
        "    \"\"\"\n",
        "    results = regressor.evals_result()\n",
        "    epochs = len(results['validation_0']['mae'])\n",
        "    x_axis = range(0, epochs)\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.grid(True, which='major')\n",
        "    ax.plot(x_axis, results['validation_0']['mae'],\n",
        "            label='Train')\n",
        "    ax.plot(x_axis, results['validation_1']['mae'],\n",
        "            label='Test')\n",
        "    ax.legend()\n",
        "    plt.ylabel('Error')\n",
        "    plt.title('Error')\n",
        "    plt.show()\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "\n",
        "def shap_calculations_xgb(regressor, XX_df):\n",
        "    \"\"\"\n",
        "    Calculate shap vales for the xgb_regressor and the dependent variables.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    regressor : Xgboost regressor.\n",
        "    XX_df : Dependent variables.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None.\n",
        "\n",
        "    \"\"\"\n",
        "    explainer = shap.TreeExplainer(regressor)\n",
        "\n",
        "    shap_values = explainer.shap_values(XX_df)\n",
        "    plt.set_cmap(\"plasma\")\n",
        "    shap.summary_plot(shap_values,\n",
        "                      XX_df,\n",
        "                      plot_type=\"violin\",\n",
        "\n",
        "                      max_display=10,\n",
        "                      show=False)\n",
        "\n",
        "    #plt.xlabel(\"Mikilvægi óháðra breyta til hækkunar (vinstra megin) og lækkunar (hægra megin)\")\n",
        "    plt.ylabel(\"Nöfn óháðu breytanna, (lengdir í cm, fjöldi fiska í rallinu og magn af veiði)\")\n",
        "    plt.xlabel(\"Tonn\")\n",
        "    #plt.yticks(fontsize=2)\n",
        "    plt.gcf().axes[-1].set_ylabel('Rauður litur þýðir lækkun, blár litur þýðir hækkun')\n",
        "    plt.gcf().axes[-1].set_aspect(100)\n",
        "    plt.gcf().axes[-1].set_box_aspect(100)\n",
        "    plt.gcf().axes[-1 ].set_yticklabels(['Lágt', 'Hátt'])\n",
        "    plt.show()\n",
        "\n",
        "    shap_values = explainer(XX_df)\n",
        "    shap.waterfall_plot(shap_values[34], max_display=15)\n",
        "    f = shap.plots.force(explainer.expected_value, shap_values.values, XX_df)\n",
        "    shap.save_html(\"index.htm\", f)\n",
        "    shap.plots.scatter(shap_values[:, 51])\n",
        "    plt.show\n",
        "    shap_interaction_values = shap.TreeExplainer(\n",
        "        regressor).shap_interaction_values(XX_df.iloc[:2000, :])\n",
        "    shap.dependence_plot(\n",
        "        (\"max(cum)\", \"number\"),\n",
        "        shap_interaction_values,\n",
        "        XX_df)\n",
        "    plt.show\n",
        "\n",
        "\n",
        "def svr_regression(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Regress using support vector machine.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : Dependent training variables.\n",
        "    y_train : Independent training variables.\n",
        "    X_test : Dependent testing variables.\n",
        "    y_test : Independent testing variables.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Prediction for test variables.\n",
        "\n",
        "    \"\"\"\n",
        "    pipe_svr = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        SVR())\n",
        "\n",
        "    param_range = [.01, .1, 10, 100]\n",
        "    param_grid = [{\n",
        "        'svr__C': param_range,\n",
        "        'svr__kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
        "        'svr__epsilon': param_range,\n",
        "        'svr__coef0': param_range,\n",
        "        'svr__gamma': param_range}]\n",
        "\n",
        "    svr_regressor = GridSearchCV(estimator=pipe_svr,\n",
        "                                 param_grid=param_grid,\n",
        "                                 scoring='neg_mean_absolute_error',\n",
        "                                 n_jobs=-1,\n",
        "                                 verbose=1)\n",
        "\n",
        "    svr_regressor.fit(X_train, y_train)\n",
        "    print(svr_regressor.best_estimator_)\n",
        "    return svr_regressor.predict(X_test)\n",
        "\n",
        "\n",
        "def regression_over_possible_values_XGB(X, y, interval_int, parameters):\n",
        "    \"\"\"\n",
        "    Loop over possible stock sizes, regressing in every step.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : Dataframe with dependent variables.\n",
        "    y : Dataframe with independent variables.\n",
        "    interval_int : step interval.\n",
        "\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    result_dict : json string with solutions in each interval.\n",
        "\n",
        "    \"\"\"\n",
        "    test_size = .20\n",
        "    seed = 5\n",
        "    result_dict = {'fjoldi2022': [], 'fjoldi2021': [],\n",
        "                   'fjoldi2020': [], 'mae': [], 'rmse': [], 'r2': [],\n",
        "                   'evs': [], 'regressor':[]}\n",
        "\n",
        "    xgb1 = xgb.XGBRegressor(objective='reg:squarederror', seed=seed)\n",
        "\n",
        "    for add_int in range(0, 110000, interval_int):\n",
        "        print(add_int)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X,\n",
        "            y,\n",
        "            test_size=test_size,\n",
        "            random_state=seed)\n",
        "\n",
        "        n_iter = 200\n",
        "        n_iter = n_iter\n",
        "\n",
        "        xgb_regressor = GridSearchCV(xgb1,\n",
        "                                     parameters,\n",
        "                                     n_jobs=-1,\n",
        "                                     cv=3,\n",
        "                                     verbose=1)\n",
        "\n",
        "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
        "\n",
        "        xgb_regressor.fit(X_train,\n",
        "                          y_train,\n",
        "                          eval_set=eval_set,\n",
        "                          verbose=False)\n",
        "\n",
        "        y_pred_test = xgb_regressor.predict(X_test)\n",
        "\n",
        "        # y_pred_test = svr_regression(X_train, y_train, X_test, y_test)\n",
        "\n",
        "        result_dict['fjoldi2022'].append(y.iloc[37])\n",
        "        result_dict['fjoldi2021'].append(y.iloc[36])\n",
        "        result_dict['fjoldi2020'].append(y.iloc[35])\n",
        "        result_dict['regressor'].append(xgb_regressor)\n",
        "\n",
        "        result_dict['mae'].append(mean_absolute_error(y_test,\n",
        "                                                      y_pred_test))\n",
        "        result_dict['rmse'].append(math.sqrt(mean_squared_error(y_test,\n",
        "                                                                y_pred_test)))\n",
        "        result_dict['r2'].append(r2_score(y_test,\n",
        "                                          y_pred_test))\n",
        "        result_dict['evs'].append(explained_variance_score(y_test,\n",
        "                                                           y_pred_test))\n",
        "        y.iat[35] += interval_int * (y.iloc[35]/y.iloc[36])\n",
        "        y.iat[36] += interval_int * (y.iloc[36]/y.iloc[37])\n",
        "        y.iat[37] += interval_int\n",
        "\n",
        "    min_value = min(result_dict['mae'])\n",
        "    min_index = result_dict['mae'].index(min_value)\n",
        "\n",
        "    y.iat[35] = result_dict['fjoldi2020'][min_index]\n",
        "    y.iat[36] = result_dict['fjoldi2021'][min_index]\n",
        "    y.iat[37] = result_dict['fjoldi2022'][min_index]\n",
        "\n",
        "    regressor = GridSearchCV(xgb1,\n",
        "                             parameters,\n",
        "                             n_jobs=-1,\n",
        "                             cv=3,\n",
        "                             verbose=0)\n",
        "\n",
        "    regressor.fit(X, y)\n",
        "    params = regressor.best_params_\n",
        "    regressor = xgb.XGBRegressor(**params)\n",
        "    regressor.fit(X, y)\n",
        "\n",
        "    shap_calculations_xgb(regressor, X)\n",
        "\n",
        "    print(regressor.predict(X))\n",
        "\n",
        "    return result_dict\n",
        "\n",
        "# %%\n",
        "def free_regression_XGB(X, y, parameters, start_year, end_year):\n",
        "    \"\"\"\n",
        "    Regression where part of y values are free.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : Dataframe with dependent variables.\n",
        "    y : Dataframe with independent variables.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    returns array with predection for y based intiial values of X and\n",
        "    the regression.\n",
        "\n",
        "    \"\"\"\n",
        "    seed = 3\n",
        "\n",
        "    xgb1 = xgb.XGBRegressor(objective='reg:squarederror',\n",
        "                            seed=seed)\n",
        "\n",
        "\n",
        "\n",
        "    X_train = pd.concat([X.iloc[:start_year, :], X.iloc[end_year:, :]])\n",
        "    y_train = pd.concat([y.iloc[:start_year], y.iloc[end_year:]])\n",
        "    X_test = X.iloc[start_year:end_year, :]\n",
        "    y_test = y.iloc[start_year:end_year]\n",
        "\n",
        "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
        "\n",
        "    regressor = GridSearchCV(xgb1,\n",
        "                             parameters,\n",
        "                             n_jobs=-1,\n",
        "                             cv=3,\n",
        "                             verbose=False)\n",
        "\n",
        "    regressor.fit(X_train,\n",
        "                  y_train,\n",
        "                  eval_set=eval_set,\n",
        "                  verbose=1)\n",
        "\n",
        "    # params = regressor.best_params_\n",
        "    # regressor = xgb.XGBRegressor(**params)\n",
        "    # regressor.fit(X, y)\n",
        "    fitting_plot(y_test,\n",
        "                 regressor.predict(X_test),\n",
        "                 X_test,\n",
        "                 regressor)\n",
        "\n",
        "    return regressor.predict(X)\n",
        "# %%\n",
        "def slack_XGB(X, y, parameters):\n",
        "    \"\"\"\n",
        "    Regression where part of y values are free.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : Dataframe with dependent variables.\n",
        "    y : Dataframe with independent variables.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    returns array with predection for y based intiial values of X and\n",
        "    the regression.\n",
        "\n",
        "    \"\"\"\n",
        "    seed = 5\n",
        "\n",
        "    xgb1 = xgb.XGBRegressor(objective='reg:squarederror',\n",
        "                            seed=seed)\n",
        "\n",
        "\n",
        "    regressor = GridSearchCV(xgb1,\n",
        "                             parameters,\n",
        "                             n_jobs=-1,\n",
        "                             cv=3,\n",
        "                             verbose=False)\n",
        "\n",
        "    regressor.fit(X,\n",
        "                  y,\n",
        "                  verbose=False)\n",
        "\n",
        "    error_plot(regressor)\n",
        "    # params = regressor.best_params_\n",
        "    # regressor = xgb.XGBRegressor(**params)\n",
        "    # regressor.fit(X, y)\n",
        "\n",
        "    return regressor.predict(X)\n",
        "# %%\n",
        "def plot_result_range(result_dict, interval_int, fractile, y_offset):\n",
        "    \"\"\"\n",
        "    Plot mae and r^2 for the range of the looped regression.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    result_dict : a dictianary containing the results from different regression\n",
        "    over the interval.\n",
        "    interval_int : integer with the increments used over the interval.\n",
        "    fractile : fractile used for Winsorization.\n",
        "\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None.\n",
        "\n",
        "    \"\"\"\n",
        "    result_dict['x'] = range(379+y_offset,\n",
        "                             489+y_offset,\n",
        "                             int(interval_int/1e3))\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.set(style='whitegrid',\n",
        "            palette='pastel', )\n",
        "    sns.lineplot(x='x',\n",
        "                 y='r2',\n",
        "                 data=result_dict,\n",
        "                 color=\"red\",\n",
        "                 ax=ax)\n",
        "    ax.set(xlabel='Þúsundir tonna',\n",
        "           ylabel='Fervik sem skýrast af óháðum breytum, r2,  eru sýnd með rauðum ferli',\n",
        "           # title='Leiðrétting Winsor notar '+fractile+' hlutfallsbrot.',\n",
        "           ylim=(0, 1))\n",
        "\n",
        "    ax2 = ax.twinx()\n",
        "    sns.lineplot(x='x',\n",
        "                 y='mae',\n",
        "                 data=result_dict,\n",
        "                 color='blue',\n",
        "                 markers=True, ax=ax2)\n",
        "    ax2.set(ylabel='Meðaltölugildi skekkjunnar er sýnt með bláum ferli')\n",
        "    plt.show()\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "# %%\n",
        "def minimum_error_stock_assessment():\n",
        "\n",
        "    minimum_assessment_difference= 1000000\n",
        "    minimum_start_year = -1\n",
        "    minimum_end_year = -1\n",
        "    fractile = \"094\"\n",
        "    parameters_free_regression = {\n",
        "        'eval_metric': [\"mae\"],\n",
        "        'learning_rate': [0.05, .1, .3],\n",
        "        'max_depth': [2],\n",
        "        'min_child_weight': [2],\n",
        "        'subsample': [.5, 1],\n",
        "        'colsample_bytree': [.5, 1],\n",
        "        'n_estimators': [50]\n",
        "    }\n",
        "    for fractile in (\"094\",\"096\",\"098\"):\n",
        "        for start_year in range(20,24):\n",
        "            for end_year in range (30,34):\n",
        "\n",
        "                (X_observed, y_observed) = get_new_data(fractile)\n",
        "                y_2022 = y_observed.iloc[37]\n",
        "\n",
        "\n",
        "\n",
        "                y_calculated = pd.Series(free_regression_XGB(X_observed,\n",
        "                                                             y_observed,\n",
        "                                                             parameters_free_regression,\n",
        "                                                             start_year,\n",
        "                                                             end_year))\n",
        "\n",
        "\n",
        "                y_offset = int((y_calculated.iloc[37]-y_2022)/1e6)\n",
        "                y_observed.index=range(0,38)\n",
        "                y_df = pd.concat([y_observed,\n",
        "                                  y_calculated],\n",
        "                                 axis=1,\n",
        "                                 join='inner')\n",
        "                y_df.columns = ['Formal stock assessment', 'Machine learning stock assessment']\n",
        "                y_df['Difference'] = y_df['Formal stock assessment'] - y_df['Machine learning stock assessment']\n",
        "                y_df = y_df.rename(index = lambda x: x + 1985)\n",
        "                assessment_difference = sum(y_df['Difference'].abs())/(end_year-start_year)\n",
        "                if assessment_difference < minimum_assessment_difference:\n",
        "                    minimum_assessment_difference = assessment_difference\n",
        "                    minimum_start_year = start_year\n",
        "                    minimum_end_year = end_year\n",
        "    return (minimum_start_year, minimum_end_year, minimum_assessment_difference, fractile )\n",
        "# %%\n",
        "# def main():\n",
        "\"\"\"\n",
        "Run main algorithms.\n",
        "\n",
        "Returns\n",
        "-------\n",
        "None.\n",
        "\n",
        "\"\"\"\n",
        "# runs regression where data has been adjusted for large catches\n",
        "fractile = '096'\n",
        "interval_int = 1000\n",
        "\n",
        "parameters = {\n",
        "    'eval_metric': [\"mae\"],\n",
        "    'learning_rate': [0.05, .1, .3],\n",
        "    'max_depth': [2],\n",
        "    'min_child_weight': [2],\n",
        "    'subsample': [.5],\n",
        "    'colsample_bytree': [.5],\n",
        "    'n_estimators': [50]\n",
        "}\n",
        "\n",
        "\n",
        "(X_observed, y_observed) = get_new_data(fractile)\n",
        "result_dict_gb = regression_over_possible_values_XGB(X_observed,\n",
        "                                                     y_observed,\n",
        "                                                     interval_int,\n",
        "                                                     parameters)\n",
        "plot_result_range(result_dict_gb,\n",
        "                  interval_int,\n",
        "                  fractile,\n",
        "                  0)\n",
        "\n",
        "# %% free regression where data has been adjusted for large catches\n",
        "fractile = \"100\"\n",
        "interval_int = 1000\n",
        "\n",
        "(X_observed, y_observed) = get_new_data(fractile)\n",
        "y_2022 = y_observed.iloc[37]\n",
        "\n",
        "parameters_free_regression = {\n",
        "    'eval_metric': [\"mae\"],\n",
        "    'learning_rate': [.1, .3],\n",
        "    'max_depth': [2],\n",
        "    'min_child_weight': [1],\n",
        "    'subsample': [.5],\n",
        "    'colsample_bytree': [.5],\n",
        "    'n_estimators': [50]\n",
        "}\n",
        "\n",
        "start_year = 29\n",
        "end_year = 33\n",
        "\n",
        "y_calculated = pd.Series(free_regression_XGB(X_observed,\n",
        "                                             y_observed,\n",
        "                                             parameters_free_regression,\n",
        "                                             start_year,\n",
        "                                             end_year))\n",
        "\n",
        "\n",
        "y_offset = int((y_calculated.iloc[37]-y_2022)/1e6)\n",
        "y_observed.index=range(0,38)\n",
        "y_df = pd.concat([y_observed,\n",
        "                  y_calculated],\n",
        "                 axis=1,\n",
        "                 join='inner')\n",
        "y_df.columns = ['Formal stock assessment', 'Machine learning stock assessment']\n",
        "y_df['Difference'] = y_df['Formal stock assessment'] - y_df['Machine learning stock assessment']\n",
        "y_df = y_df.rename(index = lambda x: x + 1985)\n",
        "y_df['mae'] = ([31982 for x in range(len(y_df.index))])\n",
        "y_df['mae-'] = - y_df['mae']\n",
        "y_df['mae++'] =y_df['Machine learning stock assessment'] + y_df['mae']\n",
        "y_df['mae--'] = y_df['Machine learning stock assessment'] - y_df['mae']\n",
        "\n",
        "\n",
        "pd.options.display.float_format = '{:,.0f}'.format\n",
        "y_df['Formal stock assessment'] = y_df['Formal stock assessment'].astype(float)\n",
        "y_df.columns = ['Stofnmæling', 'Vélanámsaðferð', 'Mismunur',\n",
        "                'Undir skekkja', 'Yfir skekkja', 'Efri skekkjumörk', 'Neðri skekkjumörk']\n",
        "y_df.iloc[13:38,:].plot(kind='line')\n",
        "sns.lineplot( y_df.iloc[13:38,:])\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.ylabel(\"Tonn\")\n",
        "plt.show()\n",
        "\n",
        "print(y_df.iloc[start_year:38, :])\n",
        "\n",
        "result_dict_gb = regression_over_possible_values_XGB(X_observed,\n",
        "                                                     y_calculated,\n",
        "                                                     interval_int,\n",
        "                                                     parameters_free_regression)\n",
        "plot_result_range(result_dict_gb,\n",
        "                  interval_int,\n",
        "                  fractile,\n",
        "                  y_offset)\n",
        "\n",
        "\n",
        "\n",
        "# %% pca analysis\n",
        "\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_sca = scaler.fit_transform(X_observed)\n",
        "pca = PCA(n_components=18)\n",
        "pca.fit(X_sca)\n",
        "print((pca.explained_variance_ratio_))\n",
        "print(pca.singular_values_)\n",
        "\n",
        "# %% simple prediction using all data\n",
        "\n",
        "\n",
        "\n",
        "fractile = '096'#current_prediction = 0\n",
        "L = []\n",
        "M = []\n",
        "(X_Observed, y_observed) = get_new_data(fractile)\n",
        "y_observed = y_observed.to_frame()\n",
        "\n",
        "for j in range(len(X_Observed)):\n",
        "  x_ar = X_Observed.iloc[j,:].to_frame().T\n",
        "  X_after = X_Observed.drop(X_Observed.index[j], axis = 0)\n",
        "  y = y_observed.drop(index = j, axis =0)\n",
        "  current_prediction = 0\n",
        "  for i in range(100):\n",
        "    seed = random.randint(1,100000)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_after,\n",
        "                                                        y,\n",
        "                                                        test_size=0.2,\n",
        "                                                        random_state=seed)\n",
        "\n",
        "    model = GradientBoostingRegressor(n_estimators=100,\n",
        "                                      learning_rate=0.05,\n",
        "                                      max_depth=3,\n",
        "                                      random_state=seed)\n",
        "    model.fit(X_train, y_train.values.ravel())\n",
        "    a = model.predict(x_ar)\n",
        "    current_prediction += a\n",
        "    L.append(a[0])\n",
        "  M.append(current_prediction/(i+1))\n",
        "\n",
        "\n",
        "print(f\"Fjöldi karfa = : {current_prediction[0]/(i+1)}\")\n",
        "\n",
        "\n",
        "M = [item.item() for item in M]\n",
        "print(M)\n",
        "\n",
        "plt.plot(M)\n",
        "plt.plot(y_observed, color = 'red')\n",
        "\n",
        "\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "print(\"meðalskekkja   : %.2f\" % mean_absolute_error(M, y_observed))\n",
        "print(\"meðalfrávik    : %.2f\" %\n",
        "      mean_squared_error(M, y_observed, squared=False))\n",
        "print(\"r2             : %.2f\" % r2_score(M, y_observed))\n",
        "print(\"útskýrt fervik : %.2f\" % explained_variance_score(M, y_observed))\n",
        "\n",
        "data = np.array(L[3700:])\n",
        "\n",
        "# plot histogram\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(data, bins='auto', alpha=0.7, rwidth=0.85, color='#607c8e')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram')\n",
        "\n",
        "# Q-Q plot\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(data, dist=\"norm\", plot=plt)\n",
        "plt.title('Q-Q Plot')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# load results into DataFrame\n",
        "df = pd.DataFrame(index=range(38),columns=range(100))\n",
        "for i in range (38):\n",
        "    for j in range(100):\n",
        "        df.iat[i,j] = L[j+100*i]\n",
        "\n",
        "# Find highest explained variance score and plot\n",
        "mae_min = 0\n",
        "evs_max = 0\n",
        "df_max = 0\n",
        "for j in range(100):\n",
        "    evs = explained_variance_score(y_observed, df[j])\n",
        "    if  evs > evs_max:\n",
        "        evs_max = evs\n",
        "        mae_min = mean_absolute_error(y_observed, df[j])\n",
        "        df_max = j\n",
        "\n",
        "\n",
        "plt.plot(df[df_max])\n",
        "plt.plot(y_observed, color = 'red')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(evs_max, mae_min)\n",
        "\n",
        "# smá leikur með shap\n",
        "model.fit(X_observed, df[df_max])\n",
        "explainer = shap.TreeExplainer(model)\n",
        "\n",
        "shap_values = explainer.shap_values(X_observed)\n",
        "plt.set_cmap(\"plasma\")\n",
        "shap.summary_plot(shap_values,\n",
        "                  X_observed,\n",
        "                  plot_type=\"violin\",\n",
        "\n",
        "                  max_display=20,\n",
        "                  show=False)\n",
        "\n",
        "#plt.xlabel(\"Mikilvægi óháðra breyta til hækkunar (vinstra megin) og lækkunar (hægra megin)\")\n",
        "plt.ylabel(\"Nöfn óháðu breytanna, (lengdir í cm, fjöldi fiska í rallinu og magn af veiði)\")\n",
        "plt.xlabel(\"Tonn\")\n",
        "#plt.yticks(fontsize=2)\n",
        "plt.gcf().axes[-1].set_ylabel('Rauður litur þýðir lækkun, blár litur þýðir hækkun')\n",
        "plt.gcf().axes[-1].set_aspect(100)\n",
        "plt.gcf().axes[-1].set_box_aspect(100)\n",
        "plt.gcf().axes[-1 ].set_yticklabels(['Lágt', 'Hátt'])\n",
        "plt.show()\n",
        "\n",
        "# shap foss for lokaspána\n",
        "explainer = shap.Explainer(model, X_observed)\n",
        "shap_values = explainer(X_observed)\n",
        "shap.waterfall_plot(shap_values[37], max_display=15)\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    main()"
      ]
    }
  ]
}